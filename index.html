<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG" />
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta
      property="og:description"
      content="SOCIAL MEDIA DESCRIPTION TAG TAG"
    />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG" />
    <meta
      name="twitter:description"
      content="TWITTER BANNER DESCRIPTION META TAG"
    />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta
      name="twitter:image"
      content="static/images/your_twitter_banner_image.png"
    />
    <meta name="twitter:card" content="summary_large_image" />
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Where Am I</title>
    <link
      rel="icon"
      type="image/x-icon"
      href="static/icons/openstreetmap.svg"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link rel="stylesheet" href="static/css/wehereami.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.10.377/pdf.min.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>

    <link
      href="https://api.mapbox.com/mapbox-gl-js/v3.8.0/mapbox-gl.css"
      rel="stylesheet"
    />
    <script src="https://api.mapbox.com/mapbox-gl-js/v3.8.0/mapbox-gl.js"></script>
  </head>
  <body>
    <!-- Paper Start -->
    <section
      class="hero"
      style="
        display: flex;
        align-items: center;
        justify-content: center;
        text-align: center;
        height: 100vh;
      "
    >
      <h1
        class="title is-1 publication-title"
        style="margin-top: 9vh; display: flex; align-items: center"
      >
        <img
          id="painting_icon1"
          width="8%"
          src="static/icons/openstreetmap.svg"
          style="
            vertical-align: middle;
            position: relative;
            width: 60px;
            height: 60px;
          "
        />
        <!-- <img
          src="static/icons/title.png"
          style="width: 300px; height: 80px; margin-left: 10px"
        /> -->
        <span class="highlight">Where am I? </span>
      </h1>

      <h1
        class="title is-2 publication-title"
        style="color: #020202; width: 35vw"
      >
        Cross-View Geo-localization with Natural Language Descriptions
      </h1>
      <div
        class="authors"
        style="
          font-size: 1.2rem;
          color: #444;
          margin: 1.5rem 0;
          text-align: center;
        "
      >
        <p>
          <a
            href="https://yejy53.github.io/"
            target="_blank"
            style="color: inherit; text-decoration: none; cursor: pointer"
            onmouseover="this.style.textDecoration='underline'"
            onmouseout="this.style.textDecoration='none'"
            >Junyan Ye</a
          ><sup>1,2*</sup>, <span style="color: inherit">Honglin Lin</span
          ><sup>2*</sup>, <span style="color: inherit">Leyan Ou</span
          ><sup>1</sup>, <span style="color: inherit">Dairong Chen</span
          ><sup>4,1</sup>, <span style="color: inherit">Zihao Wang</span
          ><sup>1</sup>,

          <a
            href="https://conghui.github.io/"
            target="_blank"
            style="color: inherit; text-decoration: none; cursor: pointer"
            onmouseover="this.style.textDecoration='underline'"
            onmouseout="this.style.textDecoration='none'"
            >Conghui He</a
          ><sup>2,3</sup>,
          <a
            href="https://liweijia.github.io/"
            target="_blank"
            style="color: inherit; text-decoration: none; cursor: pointer"
            onmouseover="this.style.textDecoration='underline'"
            onmouseout="this.style.textDecoration='none'"
            >Weijia Li</a
          ><sup>1‚Ä†</sup>
        </p>
        <p>
          <sup>1</sup
          ><a
            href="https://www.sysu.edu.cn/"
            target="_blank"
            style="color: inherit; text-decoration: none; cursor: pointer"
            onmouseover="this.style.textDecoration='underline'"
            onmouseout="this.style.textDecoration='none'"
            >Sun Yat-Sen University</a
          >, <sup>2</sup
          ><a
            href="https://www.shlab.org.cn/"
            target="_blank"
            style="color: inherit; text-decoration: none; cursor: pointer"
            onmouseover="this.style.textDecoration='underline'"
            onmouseout="this.style.textDecoration='none'"
            >Shanghai AI Laboratory</a
          >, <sup>3</sup
          ><a
            href="https://www.sensetime.com/"
            target="_blank"
            style="color: inherit; text-decoration: none; cursor: pointer"
            onmouseover="this.style.textDecoration='underline'"
            onmouseout="this.style.textDecoration='none'"
            >Sensetime Research</a
          >, <sup>4</sup
          ><a
            href="https://www.whu.edu.cn/"
            target="_blank"
            style="color: inherit; text-decoration: none; cursor: pointer"
            onmouseover="this.style.textDecoration='underline'"
            onmouseout="this.style.textDecoration='none'"
            >Wuhan University</a
          >
        </p>
      </div>
      <div
        style="
          display: flex;
          justify-content: center;
          align-items: center;
          gap: 30px;
        "
      >
        <a href="https://www.sysu.edu.cn/" target="_blank">
          <img
            src="static/icons/sysu.png"
            style="width: 120px; height: 120px"
          />
        </a>
        <a href="https://www.shlab.org.cn/" target="_blank">
          <img
            src="static/icons/ShangHaiAILab.png"
            style="width: 200px; height: 60px"
          />
        </a>
        <a href="https://www.sensetime.com/" target="_blank">
          <img
            src="static/icons/Sensetime.png"
            style="width: 120px; height: 50px"
          />
        </a>
        <a href="https://www.whu.edu.cn/" target="_blank">
          <img src="static/icons/WHU.png" style="width: 120px; height: 120px" />
        </a>
      </div>

      <div class="column has-text-centered">
        <div class="publication-links">
          <!-- Arxiv PDF link -->
          <span class="link-block">
            <a
              href="https://arxiv.org/abs/2412.17007"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
            >
              <span class="icon">
                <i class="fas fa-file-pdf"></i>
              </span>
              <span>Paper</span>
            </a>
          </span>

          <!-- Github link -->
          <span class="link-block">
            <a
              href="https://github.com/yejy53/CVG-Text"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
            >
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>

          <!-- huggingface abstract Link -->
          <span class="link-block">
            <a
              href="https://huggingface.co/datasets/CVG-Text/CVG-Text"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark"
            >
              <span class="icon" style="vertical-align: middle; font-size: 20px"
                >ü§ó</span
              >
              <span>Dataset</span>
            </a>
          </span>
        </div>
      </div>

      <img src="static/images/start.png" style="width: 40vw" />
      <h1
        style="
          width: 40%;
          font-size: 15px;
          line-height: 1.5;
          text-align: justify;
          margin: 15px auto;
          margin-top: 2vh;
        "
      >
        <strong>Towards Text-guided Geo-localization.</strong> In scenarios
        where GPS signals are interfered with, users must describe their
        surroundings using natural language, providing various location cues to
        determine their position <em>(Up)</em>. To address this, we introduce a
        text-based cross-view geo-localization task, which retrieves satellite
        imagery or OSM data based on textual queries for position localization
        <em>(Down)</em>.
      </h1>
    </section>

    <!-- End Start -->

    <!--  Abstract  -->
    <!-- <section
      class="section hero is-light"
      style="
        display: flex;
        flex-direction: column;
        align-items: center;
        text-align: center;
      "
    >
      <h2
        class="title is-3"
        style="
          width: 40vw;
          text-align: left;
          margin-bottom: 30px;
          margin-left: auto;
          margin-right: auto;
        "
      >
        Abstract
      </h2>
      <h1
        style="
          width: 40vw;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin: 20px auto;
          margin-top: 2vh;
          margin-bottom: 0vh;
        "
      >
        Cross-view geo-localization identifies the locations of street-view
        images by matching them with geo-tagged satellite images or OSM.
        However, most existing studies focus on image-to-image retrieval, with
        fewer addressing text-guided retrieval, a task vital for applications
        like pedestrian navigation and emergency response. In this work, we
        introduce a novel task for cross-view geo-localization with natural
        language descriptions, which aims to retrieve corresponding satellite
        images or OSM database based on scene text descriptions. To support this
        task, we construct the CVG-Text dataset by collecting cross-view data
        from multiple cities and employing a scene text generation approach that
        leverages the annotation capabilities of Large Multimodal Models to
        produce high-quality scene text descriptions with localization details.
        Additionally, we propose a novel text-based retrieval localization
        method, CrossText2Loc, which improves recall by 10% and demonstrates
        excellent long-text retrieval capabilities. In terms of explainability,
        it not only provides similarity scores but also offers retrieval
        reasons.
      </h1>
    </section> -->
    <!-- Abstract -->

    <!-- <div
      style="
        height: 150px;
        background: linear-gradient(to bottom, #f5f5f5, rgb(252, 246, 238));
      "
    ></div> -->

    <!-- Task Application -->
    <section
      class="hero is-small"
      style="
        display: flex;
        flex-direction: column;
        align-items: center;
        text-align: center;
      "
    >
      <h2
        class="title is-3"
        style="
          width: 40vw;
          text-align: left;
          margin-bottom: 30px;
          margin-left: auto;
          margin-right: auto;
          margin-top: 5vh;
        "
      >
        Task Application
      </h2>

      <div id="Tabdiv" style="margin-bottom: 3vh">
        <!-- Ê†áÁ≠æË°å -->
        <div class="tab-row" style="margin-bottom: 3vh">
          <div class="tab-item" id="tab-1">Passenger</div>
          <div class="tab-item" id="tab-2">Courier</div>
          <div class="tab-item" id="tab-3">Visitors</div>
          <!-- Ê†áÁ≠æÊåáÁ§∫Âô® -->
          <div class="tab-indicator"></div>
        </div>
        <div style="width: 40vw">
          <video
            class="tab-video"
            muted
            loop
            autoplay
            controls
            style="width: 100%"
          >
            <source src="static/videos/carousel1.mp4" type="video/mp4" />
          </video>
        </div>
      </div>

      <h1
        style="
          width: 40vw;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin: 20px auto;
          margin-top: 2vh;
          margin-bottom: 5vh;
        "
      >
        Text-guided geo-localization is commonly observed in daily scenarios.
        For instance, taxi drivers rely on passengers' verbal instructions to
        identify their location, tourists seek assistance from service centers
        by describing their position when lost, or delivery drivers and couriers
        are guided to specific locations through text descriptions. In
        situations where GPS or Wi-Fi signals are disrupted, describing
        locations in natural language plays a crucial role.
      </h1>
    </section>
    <!-- End Task Application -->
    <!-- <div
      style="
        height: 150px;
        background: linear-gradient(to bottom, rgb(252, 246, 238), #ffffff);
      "
    ></div> -->
    <!-- Datasets -->
    <section
      class="hero is-small"
      style="
        display: flex;
        flex-direction: column;
        align-items: center;
        text-align: center;
      "
    >
      <h2
        class="title is-3"
        style="
          width: 40vw;
          text-align: left;
          margin-bottom: 30px;
          margin-left: auto;
          margin-right: auto;
          margin-top: 5vh;
        "
      >
        Datasets
      </h2>

      <div id="Tabdiv" style="margin-bottom: 2vh">
        <!-- Ê†áÁ≠æË°å -->
        <div class="tab-row" style="margin-bottom: 2vh">
          <div class="image-tab-item" id="tab-1">
            <img
              src="static/icons/Newyork.png"
              style="width: 30px; height: 30px; margin-right: 8px"
            />
            New York
          </div>
          <div class="image-tab-item" id="tab-2">
            <img
              src="static/icons/Brisbane.png"
              style="width: 30px; height: 30px; margin-right: 8px"
            />
            Brisbane
          </div>
          <div class="image-tab-item" id="tab-3">
            <img
              src="static/icons/Tokyo.png"
              style="width: 30px; height: 30px; margin-right: 8px"
            />
            Tokyo
          </div>
          <!-- Ê†áÁ≠æÊåáÁ§∫Âô® -->
          <div class="image-tab-indicator"></div>
        </div>
        <div>
          <!-- Áî®‰∫éÊòæÁ§∫ÂõæÁâáÁöÑimgÊ†áÁ≠æ -->
          <img class="tab-image" style="width: 42vw" />
        </div>
      </div>

      <h1
        style="
          width: 40vw;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin-bottom: 30px;
        "
      >
        We introduce CVG-Text, a multimodal cross-view retrieval localization
        dataset designed to evaluate text-based scene localization tasks.
        CVG-Text covers three cities: New York, Brisbane, and Tokyo,
        encompassing over 30,000 scene data points. The data from New York and
        Tokyo is more oriented toward urban environments, while the Brisbane
        data leans towards suburban scenes. Each individual point includes
        corresponding street-view images, OSM data, satellite images, and
        associated scene text descriptions.
      </h1>

      <div
        style="
          display: flex;
          justify-content: center;
          align-items: center;
          gap: 20px;
          margin-bottom: 5vh;
        "
      >
        <img
          src="static/images/datasets_1.PNG"
          alt="Interpolate start reference image."
          style="height: 32vh"
        />
        <img
          src="static/images/datasets_2.PNG"
          alt="Interpolate start reference image."
          style="height: 32vh"
        />
      </div>

      <div
        class="map-container"
        style="
          width: 35vw;
          margin: 0 auto;
          margin-bottom: 2vh;
          position: relative;
        "
      >
        <!-- ÂüéÂ∏ÇÈÄâÊã©Âô® -->
        <div
          class="city-selector"
          style="
            position: absolute;
            top: 1vh;
            right: 1vw;
            z-index: 1;
            background-color: #ffffff;
            padding: 0.5vw;
            border-radius: 25px;
          "
        >
          <label for="city-selector" style="color: #0c0c0c; font-size: 15px"
            >Choose a city:</label
          >
          <select id="city-selector" style="width: 5vw; font-size: 15px">
            <option value="brisbane" selected>Brisbane</option>
            <option value="new-york">New York</option>
            <option value="tokyo">Tokyo</option>
          </select>
        </div>
        <!-- Âú∞ÂõæÂÆπÂô® -->
        <div id="map" style="border-radius: 25px"></div>
      </div>

      <h1
        style="
          width: 35vw;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin-bottom: 5vh;
        "
      >
        Figure illustrates the geographic distribution of sample data across the
        three cities.
      </h1>

      <!-- <h1
        style="
          width: 40vw;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin-bottom: 30px;
        "
      >
        The bellow figures illustrate examples of data from the CVG-Text dataset
        across three cities. The generated texts effectively capture the
        geolocational information present in street-view images.
      </h1> -->
    </section>
    <!-- End Datasets-->

    <!-- Method  -->
    <section
      class="section hero"
      style="
        display: flex;
        flex-direction: column;
        align-items: center;
        text-align: center;
      "
    >
      <h2
        class="title is-3"
        style="
          width: 40vw;
          text-align: left;
          margin-bottom: 30px;
          margin-left: auto;
          margin-right: auto;
        "
      >
        Method
      </h2>

      <h1
        style="
          width: 40vw;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin-bottom: 30px;
        "
      >
        In our fine-grained text synthesis, we incorporate street-view image
        input, OCR, and open-world segmentation to enhance GPT's information
        capture capability and reduce hallucination. Additionally, we carefully
        designed system prompts to guide GPT in generating fine-grained textual
        descriptions based on a progressive scene analysis chain of thought.
      </h1>

      <img
        src="static/images/datasets_3.PNG "
        style="width: 35vw; margin-bottom: 5vh"
      />

      <h1
        style="
          width: 40vw;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin-bottom: 30px;
        "
      >
        In this work, we introduce a novel task for cross-view geo-localization
        with natural language. The objective of this task is to leverage natural
        language descriptions to retrieve its corresponding OSM or satellite
        images, of which the location information is usually available. To
        address the challenge of the task, we propose the CrossText2Loc
        architecture. Our model adopts a dual-stream architecture, consisting of
        a visual encoder and a text encoder. Specifically, the architecture
        incorporates an Expanded Positional Embedding Module (EPE) and a
        contrastive learning loss to facilitate long-text contrastive learning.
        Additionally, we introduce a novel Explainable Retrieval Module (ERM),
        which includes attention heatmap generation and LMM interpretation, to
        provide natural language explanations, enhancing the interpretability of
        the retrieval process.
      </h1>

      <img
        src="static/images/Method_1.png "
        style="width: 32vw; margin-bottom: 0vh"
      />
    </section>
    <!-- End Method -->

    <!-- Experimental Results  -->
    <section
      class="section hero"
      style="
        display: flex;
        flex-direction: column;
        align-items: center;
        text-align: center;
      "
    >
      <h2
        class="title is-3"
        style="
          width: 40vw;
          text-align: left;
          margin-bottom: 30px;
          margin-left: auto;
          margin-right: auto;
        "
      >
        Experimental Results
      </h2>

      <h1
        style="
          width: 40vw;
          font-size: 30px;
          line-height: 1.5;
          text-align: center;
          margin-bottom: 30px;
          font-style: italic;
        "
      >
        Quantitative evaluation
      </h1>

      <img
        src="static/images/Quantitative evaluation.png"
        style="width: 38vw; margin-bottom: 1vh"
      />

      <h1
        style="
          width: 40vw;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin-bottom: 5vh;
        "
      >
        We evaluated the performance of various text-based retrieval methods
        under different settings of satellite images and OSM data, with the
        results shown in Table. The methods with the same architecture tend to
        perform better as the number of parameters increases. Among the existing
        approaches, BILP achieves optimal performance, as it is not constrained
        by limitations on text embedding length. Our method achieved the best
        results, outperforming the baseline CLIP method by a
        <strong>14.1%</strong> improvement in Image Recall R@1 and a
        <strong>14.8%</strong>
        improvement in Localization Recall L@50, demonstrating its superiority
        for this task.
      </h1>

      <h1
        style="
          width: 40vw;
          font-size: 30px;
          line-height: 1.5;
          text-align: center;
          margin-bottom: 3vh;
          font-style: italic;
        "
      >
        Qualitative evaluation
      </h1>

      <!-- <div style="width: 38vw; margin-bottom: 3vh">
        <canvas id="pdfCanvas"></canvas>
      </div> -->
      <img
        src="static/pdfs/Quantitative evaluation.png"
        style="width: 38vw; margin-bottom: 3vh"
      />

      <h1
        style="
          width: 40vw;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin-bottom: 10px;
        "
      >
        From the visualization results of OSM data retrieval localization in
        Figure, it is evident that our method can locate specific store
        information or similar road details based on effective textual
        descriptions. Additionally, the heatmap responses provided by the
        Explainable Retrieval Module (ERM) show which features the model focuses
        on in the retrieved image. In the first example, the model focuses on
        ‚ÄúBurger Mania‚Äù. Interestingly, even in non-top-1 results, it still
        emphasizes the burger icon. In the second example, the model focuses on
        the ‚Äúzebra crossing‚Äù and ‚Äúwhite gridline‚Äù, with the first three
        retrieval results all highlighting the zebra crossing, and the best
        retrieval result matching both features. The ERM module also leverages
        the capabilities of LMMs to provide corresponding retrieval reasoning,
        such as matching the same store information or scene features, which
        significantly enhances the interpretability of the retrieval
        localization.
      </h1>
    </section>
    <!-- End Experimental Results -->

    <!--  Conclusion  -->
    <section
      class="section hero is-light"
      style="
        display: flex;
        flex-direction: column;
        align-items: center;
        text-align: center;
      "
    >
      <h2
        class="title is-3"
        style="
          width: 40vw;
          text-align: left;
          margin-bottom: 30px;
          margin-left: auto;
          margin-right: auto;
        "
      >
        Conclusion
      </h2>
      <h1
        style="
          width: 40vw;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin-bottom: 10px;
        "
      >
        In this work, we explore the task of cross-view geo-localization using
        natural language descriptions and introduce the CVG-Text dataset, which
        includes well-aligned street-views, satellite images, OSM images, and
        text descriptions. We also propose the CrossText2Loc text retrieval
        localization method, which excels in handling long-text retrieval and
        interpretability for this task. This work represents another advancement
        in the field of natural language-based localization. It also introduces
        new application scenarios for cross-view localization, encouraging
        subsequent researchers to explore and innovate further.
      </h1>
    </section>
    <!-- End  Conclusion -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the¬†<a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                >¬†project page. You are free to borrow the source code of this
                website, we just ask that you link back to this page in the
                footer. <br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <script>
      let activeTab = 0;
      const tabItems = document.querySelectorAll(".tab-item");
      const tabIndicator = document.querySelector(".tab-indicator");
      const tabvideor = document.querySelector(".tab-video");
      const tabvideos = [
        "static/videos/Newyork_480P_100mbs_30fps.mp4",
        "static/videos/Tokyo_480p_100mbs_30fps.mp4",
        "static/videos/Fig1_480p_100mbs_30fps.mp4",
      ];
      function updateTabs() {
        tabItems.forEach((tab, index) => {
          tab.classList.remove("active");
          if (index === activeTab) {
            tab.classList.add("active");
            tabvideor.src = tabvideos[index];
          }
          const activeTabElement = tabItems[activeTab];
          const tabWidth = activeTabElement.offsetWidth;
          const tabPosition = activeTabElement.offsetLeft;
          const tabHeight = activeTabElement.offsetHeight;
          tabIndicator.style.width = `${tabWidth}px`;
          tabIndicator.style.left = `${tabPosition}px`;
          tabIndicator.style.height = `${tabHeight}px`;
        });
      }
      tabItems.forEach((item, index) => {
        item.addEventListener("click", () => {
          activeTab = index;
          updateTabs();
        });
      });

      updateTabs();

      // let activeImageTab = 0;
      // const imageTabItems = document.querySelectorAll(".image-tab-item");
      // const imageTabIndicator = document.querySelector(".image-tab-indicator");
      // const tabCanvas = document.querySelector(".tab-pdf-canvas"); // ÈÄâÊã© canvas
      // const tabPdfs = [
      //   "static/datasets/NY-text-case.pdf",
      //   "static/datasets/Brisbane-text-case.pdf",
      //   "static/datasets/Tykon-text-case.pdf",
      // ];

      // // ÂàùÂßãÂåñpdf.jsÁöÑÂ∑•‰ΩúÁ∫øÁ®ã
      // pdfjsLib.GlobalWorkerOptions.workerSrc =
      //   "https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.10.377/pdf.worker.min.js";

      // // Ê∏≤ÊüìPDFÂà∞canvas
      // function renderPdf(pdfUrl) {
      //   const loadingTask = pdfjsLib.getDocument(pdfUrl);
      //   loadingTask.promise.then(function (pdf) {
      //     // Ëé∑ÂèñÁ¨¨‰∏ÄÈ°µ
      //     pdf.getPage(1).then(function (page) {
      //       const pageWidth = page.getViewport({ scale: 1 }).width; // Ëé∑ÂèñÂéüÂßãÈ°µÈù¢ÁöÑÂÆΩÂ∫¶
      //       const pageHeight = page.getViewport({ scale: 1 }).height; // Ëé∑ÂèñÂéüÂßãÈ°µÈù¢ÁöÑÈ´òÂ∫¶

      //       // Ëé∑ÂèñcanvasÁöÑÁõÆÊ†áÂÆΩÂ∫¶Ôºà36vwÔºâ
      //       const targetWidth = window.innerWidth * 0.36; // 36vw
      //       const scale = targetWidth / pageWidth; // ËÆ°ÁÆóÁº©ÊîæÊØî‰æã

      //       // ËÆ°ÁÆóÊ∏≤ÊüìÁöÑËßÜÂè£
      //       const viewport = page.getViewport({ scale: scale });

      //       // ËÆæÁΩÆcanvasÁöÑÂ∞∫ÂØ∏‰∏∫Áº©ÊîæÂêéÁöÑÂ∞∫ÂØ∏
      //       tabCanvas.width = viewport.width;
      //       tabCanvas.height = viewport.height;

      //       const context = tabCanvas.getContext("2d");

      //       // Ê∏≤ÊüìÈ°µÈù¢Âà∞canvas
      //       const renderContext = {
      //         canvasContext: context,
      //         viewport: viewport,
      //       };
      //       page.render(renderContext).promise.then(function () {
      //         console.log("Page rendered");
      //       });
      //     });
      //   });
      // }

      // function updateImageTabs() {
      //   imageTabItems.forEach((tab, index) => {
      //     tab.classList.remove("active");
      //     if (index === activeImageTab) {
      //       tab.classList.add("active");
      //       renderPdf(tabPdfs[index]); // Ê∏≤ÊüìÈÄâ‰∏≠ÁöÑPDFÊñá‰ª∂
      //     }
      //     const activeTabElement = imageTabItems[activeImageTab];
      //     const tabWidth = activeTabElement.offsetWidth;
      //     const tabPosition = activeTabElement.offsetLeft;
      //     const tabHeight = activeTabElement.offsetHeight;
      //     imageTabIndicator.style.width = `${tabWidth}px`;
      //     imageTabIndicator.style.left = `${tabPosition}px`;
      //     imageTabIndicator.style.height = `${tabHeight}px`;
      //   });
      // }

      // imageTabItems.forEach((item, index) => {
      //   item.addEventListener("click", () => {
      //     activeImageTab = index;
      //     updateImageTabs();
      //   });
      // });

      // updateImageTabs();

      let activeImageTab = 0;
      const imageTabItems = document.querySelectorAll(".image-tab-item");
      const imageTabIndicator = document.querySelector(".image-tab-indicator");
      const tabImage = document.querySelector(".tab-image"); // ÈÄâÊã©Áî®‰∫éÊòæÁ§∫ÂõæÁâáÁöÑimgÊ†áÁ≠æ
      const tabImages = [
        "static/datasets/NY-text-case.png",
        "static/datasets/Brisbane-text-case.png",
        "static/datasets/Tykon-text-case.png",
      ];

      // Êõ¥Êñ∞ÈÄâÈ°πÂç°Áä∂ÊÄÅÂíåÂõæÁâáÊòæÁ§∫
      function updateImageTabs() {
        imageTabItems.forEach((tab, index) => {
          tab.classList.remove("active");
          if (index === activeImageTab) {
            tab.classList.add("active");
            tabImage.src = tabImages[index]; // ËÆæÁΩÆimgÊ†áÁ≠æÁöÑsrc‰∏∫ÂØπÂ∫îÁöÑÂõæÁâá
          }
          const activeTabElement = imageTabItems[activeImageTab];
          const tabWidth = activeTabElement.offsetWidth;
          const tabPosition = activeTabElement.offsetLeft;
          const tabHeight = activeTabElement.offsetHeight;
          imageTabIndicator.style.width = `${tabWidth}px`;
          imageTabIndicator.style.left = `${tabPosition}px`;
          imageTabIndicator.style.height = `${tabHeight}px`;
        });
      }

      // ‰∏∫ÊØè‰∏™ÈÄâÈ°πÂç°Ê∑ªÂä†ÁÇπÂáª‰∫ã‰ª∂ÁõëÂê¨Âô®
      imageTabItems.forEach((item, index) => {
        item.addEventListener("click", () => {
          activeImageTab = index;
          updateImageTabs();
        });
      });

      updateImageTabs();
    </script>

    <script>
      const cities = {
        "new-york": { center: [-73.976, 40.7528], zoom: 14 },
        brisbane: { center: [153.0278, -27.4698], zoom: 12 },
        tokyo: { center: [139.7517, 35.6595], zoom: 14 },
      };

      mapboxgl.accessToken =
        "pk.eyJ1IjoibHZ1c2xmY29vIiwiYSI6ImNsbzJ0Zjd2ejBiN3AydmxlN25pa3E5c3cifQ.qNGUVfyvKHJZqfKlaH_5XQ";
      const map = new mapboxgl.Map({
        container: "map",
        style: "mapbox://styles/mapbox/dark-v11",
        center: cities["brisbane"].center,
        zoom: cities["brisbane"].zoom,
      });
      map.on("load", () => {
        map.addSource("points", {
          type: "geojson",
          data: "static/locations.json",
        });
        map.addLayer({
          id: "points",
          type: "circle",
          source: "points",
          paint: {
            "circle-radius": 3,
            "circle-color": "#e88420",
          },
        });
      });

      document
        .getElementById("city-selector")
        .addEventListener("change", (event) => {
          const selectedCity = event.target.value;
          if (cities[selectedCity]) {
            map.flyTo({
              center: cities[selectedCity].center,
              zoom: cities[selectedCity].zoom,
              essential: true,
            });
          }
        });
    </script>

    <script>
      function resultRenderPdfToCanvas(
        resultPdfUrl,
        resultCanvasId,
        resultTargetWidth
      ) {
        const resultLoadingTask = pdfjsLib.getDocument(resultPdfUrl);
        resultLoadingTask.promise.then(function (resultPdf) {
          resultPdf.getPage(1).then(function (resultPage) {
            const resultOriginalViewport = resultPage.getViewport({ scale: 1 });

            const resultScale =
              resultTargetWidth / resultOriginalViewport.width;
            const resultViewport = resultPage.getViewport({
              scale: resultScale,
            });

            const resultCanvas = document.getElementById(resultCanvasId);
            const resultContext = resultCanvas.getContext("2d");

            resultCanvas.width = resultViewport.width;
            resultCanvas.height = resultViewport.height;

            // Ê∏≤ÊüìPDFÂà∞canvas
            const resultRenderContext = {
              canvasContext: resultContext,
              viewport: resultViewport,
            };
            resultPage.render(resultRenderContext).promise.then(function () {
              console.log("PDF rendered to canvas");
            });
          });
        });
      }

      // // Ë∞ÉÁî®ÂáΩÊï∞Ê∏≤ÊüìPDF
      // const resultPdfUrl = "static/pdfs/Qualitative evaluation.pdf";
      // const resultTargetWidth = window.innerWidth * 0.36; // 32vwÂØπÂ∫îÁöÑÂÆΩÂ∫¶
      // resultRenderPdfToCanvas(resultPdfUrl, "pdfCanvas", resultTargetWidth);
    </script>
  </body>
</html>
